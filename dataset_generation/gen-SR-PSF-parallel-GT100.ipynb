{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dabcd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 10:52:44.727814: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-16 10:52:57.620601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/feynman/work/dap/lcs/as274094/.local/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wf_psf as wf_psf\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed, cpu_count, parallel_backend\n",
    "\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17b37d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "\n",
    "# SED folder path\n",
    "# SED_path = '/feynman/work/dap/lcs/ec270266/wf-psf/data/SEDs/save_SEDs/'\n",
    "SED_path = '/feynman/home/dap/lcs/as274094/work/wf-psf/data/SEDs/save_SEDs/'\n",
    "\n",
    "# Output saving path (in node05 of candide or $WORK space on feynman)\n",
    "# output_folder = '/feynman/work/dap/lcs/ec270266/output/interp_SEDs/'\n",
    "output_folder = '/feynman/home/dap/lcs/as274094/work/psf_dataset_generation/output/psf_dataset1/'\n",
    "\n",
    "# Reference dataset PATH\n",
    "# reference_data = '../interp_SED_data/reference_dataset/'\n",
    "reference_data = '/feynman/home/dap/lcs/as274094/work/wf-psf/data/coherent_euclid_dataset/'\n",
    "ref_train = 'train_Euclid_res_2000_TrainStars_id_001.npy'\n",
    "ref_test  = 'test_Euclid_res_id_001.npy'\n",
    "selected_id_SED_path = 'selected_id_SED.npy'\n",
    "\n",
    "# Number of cpus to use for parallelization\n",
    "n_cpus = 512 #verify that it doesn't reach the N of actual CPUs\n",
    "\n",
    "# Save output prints to logfile\n",
    "old_stdout = sys.stdout\n",
    "log_file = open(output_folder + 'output.log','w')\n",
    "sys.stdout = log_file\n",
    "print('Starting the log file.')\n",
    "\n",
    "# Dataset ID\n",
    "dataset_id = 1\n",
    "dataset_id_str = '%03d'%(dataset_id)\n",
    "\n",
    "# This list must be in order from bigger to smaller\n",
    "n_star_list = [52000]\n",
    "n_test_stars = 20000 \n",
    "# Total stars\n",
    "n_stars = n_star_list[0] + n_test_stars\n",
    "# Max train stars\n",
    "tot_train_stars = n_star_list[0]\n",
    "\n",
    "# Parameters\n",
    "d_max = 2\n",
    "max_order = 45\n",
    "x_lims = [0, 1e3]\n",
    "y_lims = [0, 1e3]\n",
    "grid_points = [4, 4]\n",
    "n_bins = 100\n",
    "auto_init = False\n",
    "verbose = True\n",
    "\n",
    "oversampling_rate = 3.\n",
    "output_Q = 3.\n",
    "\n",
    "max_wfe_rms = 0.1\n",
    "output_dim = 32\n",
    "LP_filter_length = 2\n",
    "euclid_obsc = True\n",
    "\n",
    "# Values for getting 3xEuclid_resolution PSFs outputs.\n",
    "original_out_Q = output_Q\n",
    "original_out_dim = output_dim\n",
    "super_out_Q = 1\n",
    "super_out_res = 64\n",
    "\n",
    "# Desired WFE resolutions\n",
    "WFE_resolutions = [256]\n",
    "\n",
    "print('\\nInit dataset generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0cd8d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Star positions selected\n",
      " - Number of available CPUs: 1\n",
      " - Total number of processes: 11\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s finished\n",
      "\n",
      "All stars generated in 11.992624044418335 seconds\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "zernikes_multires = []\n",
    "sim_PSF_toolkit_multires = []\n",
    "gen_poly_fieldPSF_multires = []\n",
    "\n",
    "for i, pupil_diameter_ in tqdm(enumerate(WFE_resolutions)):\n",
    "\n",
    "    # Generate Zernike maps in max resolution\n",
    "    zernikes_multires.append(\n",
    "        wf_psf.utils.zernike_generator(\n",
    "            n_zernikes=max_order,\n",
    "            wfe_dim=pupil_diameter_\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Initialize PSF simulator for each cpu available \n",
    "    # (no euclid obscurations and wfr_rms init)\n",
    "    packed_PSFToolkit = [wf_psf.SimPSFToolkit(\n",
    "        zernikes_multires[i],\n",
    "        max_order=max_order,\n",
    "        max_wfe_rms=max_wfe_rms,\n",
    "        oversampling_rate=oversampling_rate,\n",
    "        output_Q=output_Q,\n",
    "        output_dim=output_dim,\n",
    "        pupil_diameter=pupil_diameter_,\n",
    "        euclid_obsc=euclid_obsc,\n",
    "        LP_filter_length=LP_filter_length\n",
    "    ) for j in range(n_cpus)]\n",
    "    sim_PSF_toolkit_multires.append(packed_PSFToolkit)\n",
    "\n",
    "    # Initialize one PSF field for each resolution\n",
    "    packed_polyField_PSF = [wf_psf.GenPolyFieldPSF(\n",
    "        sim_PSF_toolkit_multires[i][j],\n",
    "        d_max=d_max,\n",
    "        grid_points=grid_points,\n",
    "        max_order=max_order,\n",
    "        x_lims=x_lims,\n",
    "        y_lims=y_lims,\n",
    "        n_bins=n_bins,\n",
    "        lim_max_wfe_rms=max_wfe_rms,\n",
    "        auto_init=auto_init,\n",
    "        verbose=verbose\n",
    "    ) for j in range(n_cpus)]\n",
    "    gen_poly_fieldPSF_multires.append(packed_polyField_PSF)\n",
    "    \n",
    "\n",
    "# # Dummy SimPSFToolkit to init obscurations\n",
    "# init_toolkit = []\n",
    "# init_polyField = []\n",
    "# for i, pupil_diameter_ in enumerate(WFE_resolutions):\n",
    "#     init_toolkit.append( wf_psf.SimPSFToolkit(\n",
    "#         zernikes_multires[i], max_order=max_order, max_wfe_rms=max_wfe_rms, oversampling_rate=oversampling_rate,\n",
    "#         output_Q=output_Q, output_dim=output_dim, pupil_diameter=pupil_diameter_, euclid_obsc=euclid_obsc,\n",
    "#         LP_filter_length=LP_filter_length) )\n",
    "#     init_polyField.append( wf_psf.GenPolyFieldPSF(init_toolkit[i], d_max=d_max,\n",
    "#         grid_points=grid_points, max_order=max_order,\n",
    "#         x_lims=x_lims, y_lims=y_lims, n_bins=n_bins,\n",
    "#         lim_max_wfe_rms=max_wfe_rms, auto_init=True, verbose=verbose))\n",
    "\n",
    "# # Share C_poly coefficients throughout all the different resolution models\n",
    "# for i in range(len(gen_poly_fieldPSF_multires)):\n",
    "#     for j in range(n_cpus):\n",
    "#         gen_poly_fieldPSF_multires[i][j].set_C_poly(init_polyField[0].C_poly)\n",
    "#         gen_poly_fieldPSF_multires[i][j].set_WFE_RMS(init_polyField[0].WFE_RMS)\n",
    "#         gen_poly_fieldPSF_multires[i][j].sim_psf_toolkit.obscurations = init_toolkit[i].obscurations\n",
    "\n",
    "# Load the SEDs\n",
    "stellar_SEDs = np.load(SED_path + 'SEDs.npy', allow_pickle=True)\n",
    "stellar_lambdas = np.load(SED_path + 'lambdas.npy', allow_pickle=True)\n",
    "\n",
    "# Load reference dataset\n",
    "train_dataset_ref = np.load(reference_data+ref_train, allow_pickle=True)[()]\n",
    "test_dataset_ref = np.load(reference_data+ref_test, allow_pickle=True)[()]\n",
    "\n",
    "'''\n",
    "# Load all the stars positions\n",
    "pos_np = np.vstack((train_dataset_ref['positions'],test_dataset_ref['positions']))\n",
    "\n",
    "# Assign preselected SEDs\n",
    "selected_id_SED = np.load(reference_data+'selected_id_SED.npy', allow_pickle=True)\n",
    "SED_list = []\n",
    "for it in range(n_stars):\n",
    "    concat_SED_wv = np.concatenate((\n",
    "        stellar_lambdas.reshape(-1,1),\n",
    "        stellar_SEDs[selected_id_SED[it],:].reshape(-1,1)\n",
    "    ), axis=1)\n",
    "    SED_list.append(concat_SED_wv)\n",
    "'''\n",
    "    \n",
    "\n",
    "# Choose the locations randomly\n",
    "pos_np = np.random.rand(n_stars, 2)\n",
    "\n",
    "pos_np[:,0] = pos_np[:,0]*(x_lims[1] - x_lims[0]) + x_lims[0]\n",
    "pos_np[:,1] = pos_np[:,1]*(y_lims[1] - y_lims[0]) + y_lims[0]    \n",
    "\n",
    "# Select random SEDs\n",
    "SED_list = []\n",
    "SED_id_list = []\n",
    "for it in range(n_stars):\n",
    "    selected_id_SED = np.random.randint(low=0, high=13)\n",
    "    concat_SED_wv = np.concatenate((\n",
    "        stellar_lambdas.reshape(-1,1),\n",
    "        stellar_SEDs[selected_id_SED,:].reshape(-1,1)), axis=1)\n",
    "    SED_id_list.append(selected_id_SED)\n",
    "    SED_list.append(concat_SED_wv)\n",
    "        \n",
    "\n",
    "\n",
    "# Load and assign the C_poly matrix\n",
    "C_poly = train_dataset_ref['C_poly']\n",
    "for i in range(len(gen_poly_fieldPSF_multires)):\n",
    "    for j in range(n_cpus):\n",
    "        gen_poly_fieldPSF_multires[i][j].set_C_poly(C_poly)\n",
    "\n",
    "print('\\nStar positions selected')\n",
    "\n",
    "#######################################\n",
    "#            PARALELLIZED             #\n",
    "#######################################\n",
    "\n",
    "# Total number of stars\n",
    "n_procs = n_stars*len(WFE_resolutions)\n",
    "\n",
    "# Print some info\n",
    "cpu_info = ' - Number of available CPUs: {}'.format(n_cpus)\n",
    "proc_info = ' - Total number of processes: {}'.format(n_procs)\n",
    "print(cpu_info)\n",
    "print(proc_info)\n",
    "\n",
    "# Generate star list\n",
    "star_id_list = [id_ for id_ in range(n_stars)]\n",
    "\n",
    "\n",
    "# Function to get (i,j) from id\n",
    "def unwrap_id(id, n_stars):\n",
    "    i = int(id/n_stars)\n",
    "    j = int(id - i * n_stars)\n",
    "    return i, j\n",
    "\n",
    "def print_status(star_id, i, j):\n",
    "    print('\\nStar ' +str(star_id)+ ' done!' + '   index=('+str(i)+','+str(j)+')')\n",
    "\n",
    "# Get batches from a list\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "# Function to get one PSF\n",
    "def simulate_star(star_id, gen_poly_fieldPSF_multires,i):\n",
    "    i_,j_ = unwrap_id(star_id, n_cpus)\n",
    "    _psf, _zernike, _ = gen_poly_fieldPSF_multires[i][j_].get_poly_PSF(\n",
    "        xv_flat=pos_np[star_id, 0],\n",
    "        yv_flat=pos_np[star_id, 1],\n",
    "        SED=SED_list[star_id]\n",
    "    )\n",
    "    # Change output parameters to get the super resolved PSF\n",
    "    gen_poly_fieldPSF_multires[i][j_].sim_psf_toolkit.output_Q = super_out_Q\n",
    "    gen_poly_fieldPSF_multires[i][j_].sim_psf_toolkit.output_dim = super_out_res\n",
    "    super_psf, _, _ = gen_poly_fieldPSF_multires[i][j_].get_poly_PSF(\n",
    "        xv_flat=pos_np[star_id, 0],\n",
    "        yv_flat=pos_np[star_id, 1],\n",
    "        SED=SED_list[star_id]\n",
    "    )\n",
    "    # Put back original parameters\n",
    "    gen_poly_fieldPSF_multires[i][j_].sim_psf_toolkit.output_Q = original_out_Q\n",
    "    gen_poly_fieldPSF_multires[i][j_].sim_psf_toolkit.output_dim = original_out_dim\n",
    "    #print_status(star_id, i, star_id)\n",
    "    return (star_id, _psf, _zernike, super_psf)\n",
    "\n",
    "# Measure time\n",
    "start_time = time.time()\n",
    "\n",
    "zernike_coef_multires = []\n",
    "poly_psf_multires = []\n",
    "index_multires = []\n",
    "super_psf_multires = []\n",
    "\n",
    "for i in range(len(WFE_resolutions)):\n",
    "    index_i_list = []\n",
    "    psf_i_list = []\n",
    "    z_coef_i_list = []\n",
    "    super_psf_i_list = []\n",
    "    for batch in chunker(star_id_list, n_cpus):\n",
    "        with parallel_backend(\"loky\", inner_max_num_threads=1):\n",
    "            results = Parallel(n_jobs=n_cpus, verbose=100)(delayed(simulate_star)(_star_id, gen_poly_fieldPSF_multires,i)\n",
    "                                                for _star_id in batch)\n",
    "        index_batch,psf_batch,z_coef_batch,super_psf_batch = zip(*results)\n",
    "        index_i_list.extend(index_batch)\n",
    "        psf_i_list.extend(psf_batch)\n",
    "        z_coef_i_list.extend(z_coef_batch)\n",
    "        super_psf_i_list.extend(super_psf_batch)\n",
    "\n",
    "    index_multires.append(np.array(index_i_list) )\n",
    "    poly_psf_multires.append(np.array( psf_i_list)) \n",
    "    zernike_coef_multires.append(np.array(z_coef_i_list))\n",
    "    super_psf_multires.append(np.array(super_psf_i_list))\n",
    "\n",
    "end_time = time.time()\n",
    "print('\\nAll stars generated in '+ str(end_time-start_time) +' seconds')\n",
    "\n",
    "#######################################\n",
    "#            END PARALLEL             #\n",
    "#######################################\n",
    "\n",
    "# Add noise to generated train star PSFs and save datasets\n",
    "\n",
    "# SNR varying randomly from 50 to 400 - shared over all WFE resolutions\n",
    "rand_SNR_train = (np.random.rand(tot_train_stars) * 350) + 50\n",
    "# Copy the training stars\n",
    "train_stars = np.copy(np.array(poly_psf_multires[0])[:tot_train_stars, :, :])\n",
    "# Add Gaussian noise to the observations\n",
    "noisy_train_stars = np.stack([wf_psf.utils.add_noise(_im, desired_SNR=_SNR) \n",
    "                              for _im, _SNR in zip(train_stars, rand_SNR_train)], axis=0)\n",
    "# Generate Gaussian noise patterns to be shared over all datasets (but not every star)\n",
    "noisy_train_patterns = noisy_train_stars - train_stars\n",
    "\n",
    "\n",
    "# Add noise to generated test star PSFs and save datasets\n",
    "\n",
    "# SNR varying randomly from 20 to 400 - shared over all WFE resolutions\n",
    "rand_SNR_test = (np.random.rand(n_test_stars) * 380) + 20\n",
    "# Copy the test stars\n",
    "test_stars = np.copy(np.array(poly_psf_multires[0])[tot_train_stars:, :, :])\n",
    "# Add Gaussian noise to the observations\n",
    "noisy_test_stars = np.stack([wf_psf.utils.add_noise(_im, desired_SNR=_SNR) \n",
    "                              for _im, _SNR in zip(train_stars, rand_SNR_test)], axis=0)\n",
    "# Generate Gaussian noise patterns to be shared over all datasets (but not every star)\n",
    "noisy_test_patterns = noisy_test_stars - test_stars\n",
    "\n",
    "\n",
    "WFE_res_id = 0\n",
    "\n",
    "# Generate datasets for every WFE resolution\n",
    "for poly_psf_np, zernike_coef_np, super_psf_np in zip(poly_psf_multires, zernike_coef_multires, super_psf_multires):\n",
    "    \n",
    "    # Generate numpy array from the SED list\n",
    "    SED_np = np.array(SED_list)\n",
    "\n",
    "    # Add same noise dataset to each WFE-resolution dataset\n",
    "    noisy_train_stars = np.copy(poly_psf_np[:tot_train_stars, :, :]) + noisy_train_patterns\n",
    "    noisy_test_stars = np.copy(poly_psf_np[tot_train_stars:, :, :]) + noisy_test_patterns\n",
    "\n",
    "    # Save only one test dataset\n",
    "    # Build param dicitionary\n",
    "    dataset_params = {\n",
    "        'd_max':d_max,\n",
    "        'max_order':max_order,\n",
    "        'x_lims':x_lims,\n",
    "        'y_lims':y_lims,\n",
    "        'grid_points':grid_points,\n",
    "        'n_bins':n_bins,\n",
    "        'max_wfe_rms':max_wfe_rms,\n",
    "        'oversampling_rate':oversampling_rate,\n",
    "        'output_Q':output_Q,\n",
    "        'output_dim':output_dim,\n",
    "        'LP_filter_length':LP_filter_length,\n",
    "        'pupil_diameter':WFE_resolutions[WFE_res_id],\n",
    "        'euclid_obsc':euclid_obsc,\n",
    "        'n_stars':n_test_stars\n",
    "    }\n",
    "\n",
    "    # Save dataset C coefficient matrix (reproductible dataset)\n",
    "    C_poly = gen_poly_fieldPSF_multires[WFE_res_id][n_cpus-1].C_poly\n",
    "\n",
    "    test_psf_dataset = {\n",
    "        'stars' : poly_psf_np[tot_train_stars:, :, :],\n",
    "        'noisy_stars': noisy_test_stars,\n",
    "        'super_res_stars' : super_psf_np[tot_train_stars:, :, :],\n",
    "        'positions' : pos_np[tot_train_stars:, :],\n",
    "        'SEDs' : SED_np[tot_train_stars:, :, :],\n",
    "        'zernike_coef' : zernike_coef_np[tot_train_stars:, :, :],\n",
    "        'C_poly' : C_poly,\n",
    "        'parameters': dataset_params,\n",
    "        'SED_ids':SED_id_list[tot_train_stars:],\n",
    "        'SNR': rand_SNR_test\n",
    "    }\n",
    "\n",
    "    np.save(\n",
    "        output_folder + 'test_Euclid_res_' + str(n_test_stars) + '_TestStars_id_' + dataset_id_str + 'GT_100_bins.npy',\n",
    "        test_psf_dataset,\n",
    "        allow_pickle=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Save the different train datasets\n",
    "    for it_glob in range(len(n_star_list)):\n",
    "\n",
    "        n_train_stars = n_star_list[it_glob]\n",
    "\n",
    "        # Build param dicitionary\n",
    "        dataset_params = {\n",
    "            'd_max':d_max,\n",
    "            'max_order':max_order,\n",
    "            'x_lims':x_lims,\n",
    "            'y_lims':y_lims,\n",
    "            'grid_points':grid_points,\n",
    "            'n_bins':n_bins,\n",
    "            'max_wfe_rms':max_wfe_rms,\n",
    "            'oversampling_rate':oversampling_rate,\n",
    "            'output_Q':output_Q,\n",
    "            'output_dim':output_dim,\n",
    "            'LP_filter_length':LP_filter_length,\n",
    "            'pupil_diameter':WFE_resolutions[WFE_res_id],\n",
    "            'euclid_obsc':euclid_obsc,\n",
    "            'n_stars':n_train_stars\n",
    "        }\n",
    "\n",
    "        train_psf_dataset = {\n",
    "            'stars' : poly_psf_np[:n_train_stars, :, :],\n",
    "            'noisy_stars': noisy_train_stars[:n_train_stars, :, :],\n",
    "            'super_res_stars' : super_psf_np[:n_train_stars, :, :],\n",
    "            'positions' : pos_np[:n_train_stars, :],\n",
    "            'SEDs' : SED_np[:n_train_stars, :, :],\n",
    "            'zernike_coef' : zernike_coef_np[:n_train_stars, :, :],\n",
    "            'C_poly' : C_poly,\n",
    "            'parameters': dataset_params,\n",
    "            'SED_ids' : SED_id_list[:n_train_stars],\n",
    "            'SNR': rand_SNR_train\n",
    "        }\n",
    "\n",
    "\n",
    "        np.save(\n",
    "            output_folder + 'train_Euclid_res_' + str(n_train_stars) + '_TrainStars_id_' + dataset_id_str + 'GT_100_bins.npy',\n",
    "            train_psf_dataset,\n",
    "            allow_pickle=True\n",
    "        )\n",
    "\n",
    "    # Next desired resolution   \n",
    "    WFE_res_id += 1\n",
    "\n",
    "print('\\nDone!')\n",
    "\n",
    "# Close log file\n",
    "sys.stdout = old_stdout\n",
    "log_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
