{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-25 16:22:59.350131: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Script that receives an unclassified PSF dataset and returns the dataset with the according SEDs assigned by the PCA classifier\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "dataset_path = '/Users/as274094/Documents/psf_dataset4/'\n",
    "dataset_name = 'train_Euclid_2000_stars_id_004GT_350_bins.npy'\n",
    "#dataset_1 = np.load(dataset_path + dataset_name, allow_pickle=True)[()] # The dataset to classify\n",
    "#dataset_2 = np.load('/Users/as274094/Documents/psf_dataset3/test_Euclid_400_stars_id_003GT_350_bins.npy', allow_pickle=True)[()] # The other dataset in order to make the PCA\n",
    "dataset_2 = np.load('/Users/as274094/Documents/psf_dataset2/train_Euclid_res_52000_TrainStars_id_002GT_100_bins.npy', allow_pickle=True)[()]\n",
    "dataset_1 = np.load('/Users/as274094/Documents/psf_dataset2/test_Euclid_res_20000_TestStars_id_002GT_100_bins_SNR_100_400.npy', allow_pickle=True)[()]\n",
    "dataset_3 = np.load('/Users/as274094/Documents/psf_dataset2/test_Euclid_res_20000_TestStars_id_002GT_100_bins.npy', allow_pickle=True)[()]\n",
    "\n",
    "# Load the stars\n",
    "noisy_stars_1 = dataset_1['noisy_stars']\n",
    "noisy_stars_2 = dataset_2['noisy_stars']\n",
    "noisy_stars_3 = dataset_3['noisy_stars']\n",
    "\n",
    "#should I exclude the validation stars?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA decomposition\n",
    "\n",
    "fit_selection = np.concatenate((noisy_stars_1, noisy_stars_2), axis = 0)\n",
    "N_components = 30\n",
    "\n",
    "pca = PCA(n_components= N_components)\n",
    "pca.fit(fit_selection.reshape(-1, 1024))\n",
    "x_to_convert = pca.transform(noisy_stars_1.reshape(-1, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "model_path = '/Users/as274094/GitHub/Refractored_star_classifier/tensorflow_version/best_models/config1_PCA_dataset2B30/'\n",
    "classifier = tf.keras.models.load_model(model_path)\n",
    "\n",
    "def CtoSEDarray(c_values, variance):\n",
    "    sed_classes = ((c_values - 1.25) // 0.5).astype(int)\n",
    "    sed_classes = np.where((c_values < 1.25) | (c_values > 7.75), 20, sed_classes)\n",
    "    sed_classes = np.where((variance > 1.00), 20, sed_classes)\n",
    "    return sed_classes\n",
    "    \n",
    "def calculate_success_rate(confusion_matrix):\n",
    "    diagonal = np.trace(confusion_matrix)\n",
    "    diagonal_neighbors = np.sum(np.diagonal(confusion_matrix, offset=1)) + np.sum(np.diagonal(confusion_matrix, offset=-1))\n",
    "    total_classified = np.sum(confusion_matrix)\n",
    "    \n",
    "    success_rate = (diagonal + diagonal_neighbors) / total_classified\n",
    "    return success_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498/498 [==============================] - 1s 1ms/step\n",
      "Average F1 score: 0.34343918046252575\n",
      "\n",
      "Confusion matrix:\n",
      "[[337 324 200 129 112  50  19   6   1   0   0   0   0  11]\n",
      " [264 350 212 152 126  56  24  16   3   0   0   0   0  19]\n",
      " [ 68 177 206 273 240 113  55  44  24   4   0   0   0  13]\n",
      " [ 33 125 160 269 302 150  69  55  27   2   0   0   0  23]\n",
      " [ 31  35  86 150 365 246 135  80  54  21   4   0   0  24]\n",
      " [ 15   9  37  90 184 313 262 144 129  60  21   0   0   9]\n",
      " [ 14   8  16  45 109 220 243 221 156 110  56   1   0  14]\n",
      " [  8   6   7  13  51 103 197 266 291 211 120   2   0   8]\n",
      " [  6   9   9  12  43  76 128 241 292 242 148   9   0   8]\n",
      " [  8   6   2   9  13  52 100 153 268 310 254  17   0   3]\n",
      " [  3   2   3   0   9  12  10  46  72 211 665 155   0   6]\n",
      " [  3   0   0   3   1   6   2   2   9  55 335 828   1   5]\n",
      " [  0   0   1   1   0   1   1   2   1  11  10 242 958   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n",
      "\n",
      "Success rate: 0.6799723843595055\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and calculate metrics\n",
    "\n",
    "C_pred = classifier.predict(x_to_convert, verbose = 1).reshape(-1) # Predict the scalar parameter C\n",
    "class_pred = CtoSEDarray(C_pred,np.zeros_like(C_pred))\n",
    "\n",
    "f1_mean = np.mean(skm.f1_score(dataset_1['SED_ids'], class_pred, average = None)[:13])\n",
    "print('Average F1 score:', f1_mean)\n",
    "\n",
    "confusion_matrix = skm.confusion_matrix(dataset_1['SED_ids'], class_pred)\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix)\n",
    "\n",
    "success_rate = calculate_success_rate(confusion_matrix)\n",
    "print('\\nSuccess rate:', success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign SEDs\n",
    "\n",
    "concatenated_SEDs = np.load('concatenated_SEDs.npy', allow_pickle=True)[()]\n",
    "\n",
    "SED_list = []\n",
    "for spectral_class in class_pred:\n",
    "    if spectral_class == 20:\n",
    "        concat_SED = concatenated_SEDs[0] # what sould I do about the anomalies?\n",
    "        print('a')\n",
    "    else:\n",
    "        concat_SED = concatenated_SEDs[spectral_class]\n",
    "    SED_list.append(concat_SED)\n",
    "SED_array = np.array(SED_list, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new dataset\n",
    "\n",
    "dataset_1['SEDs'] = SED_array \n",
    "dataset_1['SED_ids'] = class_pred\n",
    "dataset_1['F1'] = f1_mean\n",
    "dataset_1['success_rate'] = success_rate\n",
    "\n",
    "np.save(\n",
    "        dataset_path + 'assigned_' + dataset_name,\n",
    "        dataset_1,\n",
    "        allow_pickle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6190589051319667 0.9545\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "\n",
    "assigned_dataset = np.load('/Users/as274094/Documents/psf_dataset4/assigned_train_Euclid_2000_stars_id_004GT_350_bins.npy', allow_pickle=True)[()]\n",
    "print(assigned_dataset['F1'],assigned_dataset['success_rate'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
