{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 17:37:42.904706: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.keras import layers\n",
    "import sklearn.metrics as skm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate configurations\n",
    "\n",
    "hidden_layers = [2, 3, 4]\n",
    "hidden_nodes = [26, 5, 11, 20, 32, 40]\n",
    "activation = ['sigmoid', 'relu']\n",
    "\n",
    "configurations = []\n",
    "\n",
    "for index, combination in enumerate(itertools.product(hidden_layers, hidden_nodes, activation), 1):\n",
    "    config = {\n",
    "        'config_name': f'config{index}',\n",
    "        'hidden_layers' : combination[0],\n",
    "        'hidden_nodes': combination[1],\n",
    "        'activation': combination[2]\n",
    "    }\n",
    "    configurations.append(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '/Users/as274094/Documents/psf_dataset2/'\n",
    "# data_path = '/gpfswork/rech/prk/uzk69cg/psf_dataset2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SEDlisttoC(SED_list):\n",
    "    sed_array = np.array(SED_list)\n",
    "    return sed_array*0.5 + 1.5\n",
    "\n",
    "def CtoSEDarray(c_values, variance):\n",
    "    sed_classes = ((c_values - 1.25) // 0.5).astype(int)\n",
    "    sed_classes = np.where((c_values < 1.25) | (c_values > 7.75), 20, sed_classes)\n",
    "    sed_classes = np.where((variance > 1.00), 20, sed_classes)\n",
    "    return sed_classes\n",
    "\n",
    "def calculate_success_rate(confusion_matrix):\n",
    "    diagonal = np.trace(confusion_matrix)\n",
    "    diagonal_neighbors = np.sum(np.diagonal(confusion_matrix, offset=1)) + np.sum(np.diagonal(confusion_matrix, offset=-1))\n",
    "    total_classified = np.sum(confusion_matrix)\n",
    "    \n",
    "    success_rate = (diagonal + diagonal_neighbors) / total_classified\n",
    "    return success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingCompletionCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_end(self, logs=None):\n",
    "        epochs = len(self.model.history.history['loss'])\n",
    "        final_loss = self.model.history.history['loss'][-1]\n",
    "        final_val_loss = self.model.history.history['val_loss'][-1]\n",
    "\n",
    "        print(\"Training completed. Number of epochs:\", epochs, \", Final training loss:\", final_loss, \", Final validation loss:\", final_val_loss)\n",
    "\n",
    "completion_callback = TrainingCompletionCallback()\n",
    "\n",
    "initializer = tf.keras.initializers.GlorotNormal(seed = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class full_model:\n",
    "    def __init__(self, dataset_name, configuration):\n",
    "        self.dataset_name = dataset_name\n",
    "        dataset = np.load(data_path + dataset_name + \".npy\", allow_pickle=True)[()]\n",
    "        self.PCA_components = dataset['N_components']\n",
    "        self.x_train = dataset['train_stars_pca']\n",
    "        self.x_val = dataset['validation_stars_pca']\n",
    "        self.x_test = dataset['test_stars_pca']\n",
    "        self.y_train = dataset['train_C']\n",
    "        self.y_val = dataset['validation_C']\n",
    "        self.y_test = dataset['test_C']\n",
    "        self.SED_val = dataset['validation_SEDs']\n",
    "        self.SED_test = dataset['test_SEDs']\n",
    "        self.learning = []\n",
    "        self.configuration = configuration\n",
    "\n",
    "    def create_model(self):\n",
    "        \n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(layers.Input(shape=[self.PCA_components]))\n",
    "        for n_layers in range(self.configuration['hidden_layers']):\n",
    "            self.model.add(layers.Dense(self.configuration['hidden_nodes'], activation= self.configuration['activation'], kernel_initializer= initializer))\n",
    "        self.model.add(layers.Dense(1, activation = 'linear', kernel_initializer= initializer))\n",
    "        \n",
    "    \n",
    "    def train_model(self, learning_rate, training_epochs, patience_epochs):\n",
    "\n",
    "        self.model.compile(\n",
    "            loss = tf.keras.losses.MeanSquaredError(),\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "        )\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = patience_epochs, restore_best_weights=True)\n",
    "\n",
    "        learn = self.model.fit(self.x_train, self.y_train, epochs= training_epochs, verbose = 0,\n",
    "                                        callbacks = [completion_callback,early_stopping], validation_data=(self.x_val,self.y_val), shuffle=True) \n",
    "\n",
    "        self.learning.append(learn)\n",
    "\n",
    "    def predict_test(self, verbose = True):\n",
    "        C_pred = self.model.predict(self.x_test, verbose = 0).reshape(-1)\n",
    "        SED_pred_test = CtoSEDarray(C_pred,np.zeros_like(C_pred))\n",
    "\n",
    "        self.mse_test = np.mean((self.y_test - C_pred)**2)\n",
    "        self.f1_test = skm.f1_score(self.SED_test, SED_pred_test, average = None)\n",
    "        self.f1_mean_test = np.mean(self.f1_test[:13])\n",
    "        self.confusion_matrix_test = skm.confusion_matrix(self.SED_test, SED_pred_test)\n",
    "        self.success_rate_test = calculate_success_rate(self.confusion_matrix_test)\n",
    "\n",
    "        if(verbose):\n",
    "            print(\"Prediction results for the test set\")\n",
    "            print('MSE:', self.mse_test)\n",
    "            print('\\nF1 score for each class:', self.f1_test)\n",
    "            print('Average F1 score:', self.f1_mean_test)\n",
    "            print(\"\\nConfusion matrix:\")\n",
    "            print(self.confusion_matrix_test)\n",
    "            print('\\nSuccess rate:', self.success_rate_test)\n",
    "\n",
    "\n",
    "    def predict_val(self, verbose = True):\n",
    "        C_pred = self.model.predict(self.x_val, verbose = 0).reshape(-1)\n",
    "        SED_pred_val = CtoSEDarray(C_pred,np.zeros_like(C_pred))\n",
    "\n",
    "        self.mse_val = np.mean((self.y_val - C_pred)**2) \n",
    "        self.f1_val = skm.f1_score(self.SED_val, SED_pred_val, average = None)\n",
    "        self.f1_mean_val = np.mean(self.f1_val[:13])\n",
    "        self.confusion_matrix_val = skm.confusion_matrix(self.SED_val, SED_pred_val)\n",
    "        self.success_rate_val = calculate_success_rate(self.confusion_matrix_val)\n",
    "\n",
    "        if(verbose):\n",
    "            print(\"Prediction results for the validation set\")\n",
    "            print('MSE:', self.mse_val)\n",
    "            print('\\nF1 score for each class:', self.f1_val)\n",
    "            print('Average F1 score:', self.f1_mean_val)\n",
    "            print(\"\\nConfusion matrix:\")\n",
    "            print(self.confusion_matrix_val)\n",
    "            print('\\nSuccess rate:', self.success_rate_val)\n",
    "\n",
    "    def save_model(self, N_model):\n",
    "\n",
    "        self.model.save(f\"saved_models/{self.configuration['config_name']}/{self.dataset_name}/my_model_{N_model}.h5\")\n",
    "\n",
    "    def load_model(self, N_model):\n",
    "        self.model = tf.keras.models.load_model(f\"saved_models/{self.configuration['config_name']}/{self.dataset_name}/my_model_{N_model}.h5\")\n",
    "        \n",
    "\n",
    "    def plot_loss(self):\n",
    "    # Plot the loss function evolution\n",
    "\n",
    "        loss_evolution = self.learning[-1].history[\"loss\"]\n",
    "        val_loss_evolution = self.learning[-1].history[\"val_loss\"]\n",
    "\n",
    "        plt.figure(figsize = (9,5))\n",
    "        plt.plot(loss_evolution,label = \"Train set\")\n",
    "        plt.plot(val_loss_evolution,label = \"Validation set\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss function value\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Loss function evolution\")\n",
    "        print(\"Training loss:\", loss_evolution[-1], \", Validation loss:\", val_loss_evolution[-1])\n",
    "\n",
    "    def plot_cf_matrix(self):\n",
    "        # Plot the confusion matrix\n",
    "\n",
    "        star_class_labels = ['O5','B0','B5','A0','A5','F0','F5','G0','G5','K0','K5','M0','M5']\n",
    "\n",
    "        plt.figure(figsize= (12,10))\n",
    "        heatmap = plt.imshow(self.confusion_matrix_test[:13,:], cmap='Blues')\n",
    "        plt.xticks(np.arange(14), star_class_labels + ['???'])\n",
    "        plt.yticks(np.arange(13), star_class_labels)\n",
    "        plt.colorbar(heatmap)\n",
    "        plt.xlabel(\"Estimated spectral type\")\n",
    "        plt.ylabel(\"True spectral type\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_array(N_components, N_model_dataset, configuration):\n",
    "    model_array = np.empty((len(N_components),N_model_dataset), dtype=object)\n",
    "\n",
    "    total_start_time = time.time() # Measure training time\n",
    "    for i in range(model_array.shape[0]):\n",
    "        print('Start of the training of PCA_dataset2B' + str(N_components[i]))\n",
    "        for j in range(model_array.shape[1]):\n",
    "            model = full_model('PCA_dataset2B' + str(N_components[i]), configuration)\n",
    "            model.create_model()\n",
    "\n",
    "            single_start_time = time.time()\n",
    "            model.train_model(training_epochs= 70, learning_rate= 0.1, patience_epochs= 10)\n",
    "            model.train_model(training_epochs= 100, learning_rate= 0.01, patience_epochs= 12)\n",
    "            single_end_time = time.time()\n",
    "            single_training_time = single_end_time - single_start_time\n",
    "            print(\"Model training time:\", single_training_time, \"seconds\")\n",
    "\n",
    "            model.predict_val(verbose=0)\n",
    "            model.predict_test(verbose=0)\n",
    "            model_array[i][j]= model\n",
    "        print('End of the training of PCA_dataset2B' + str(N_components[i]))\n",
    "\n",
    "    total_end_time = time.time()\n",
    "    total_training_time = total_end_time - total_start_time\n",
    "    print(\"Total training time:\", total_training_time, \"seconds\")\n",
    "\n",
    "    return model_array\n",
    "\n",
    "def predict_array(model_array):\n",
    "    for i in range(model_array.shape[0]):\n",
    "        for j in range(model_array.shape[1]):\n",
    "            model_array[i][j].predict_test(verbose=False)\n",
    "\n",
    "def save_model_array(model_array):\n",
    "    for i in range(model_array.shape[0]):\n",
    "        for j in range(model_array.shape[1]):\n",
    "            model = model_array[i][j]\n",
    "            model.save_model(j)\n",
    "\n",
    "def load_model_array(N_components, N_model_dataset, configuration):\n",
    "    model_array = np.empty((len(N_components), N_model_dataset), dtype=object)\n",
    "\n",
    "    for i in range(model_array.shape[0]):\n",
    "        for j in range(model_array.shape[1]):\n",
    "            model = full_model('PCA_dataset2B' + str(N_components[i]), configuration)\n",
    "            model.load_model(j)\n",
    "            model_array[i][j] = model\n",
    "\n",
    "    return model_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m configuration \u001b[39min\u001b[39;00m configurations[:\u001b[39m4\u001b[39m]:\n\u001b[1;32m      7\u001b[0m     model_array \u001b[39m=\u001b[39m load_model_array(N_components, \u001b[39m10\u001b[39m, configuration)\n\u001b[0;32m----> 8\u001b[0m     predict_array(model_array)\n\u001b[1;32m      9\u001b[0m     array_list\u001b[39m.\u001b[39mappend(model_array)\n",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m, in \u001b[0;36mpredict_array\u001b[0;34m(model_array)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(model_array\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m     31\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(model_array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[0;32m---> 32\u001b[0m         model_array[i][j]\u001b[39m.\u001b[39mpredict_test(verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[6], line 40\u001b[0m, in \u001b[0;36mfull_model.predict_test\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_test\u001b[39m(\u001b[39mself\u001b[39m, verbose \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> 40\u001b[0m     C_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_test, verbose \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m     SED_pred_test \u001b[39m=\u001b[39m CtoSEDarray(C_pred,np\u001b[39m.\u001b[39mzeros_like(C_pred))\n\u001b[1;32m     43\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmse_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_test \u001b[39m-\u001b[39m C_pred)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/psf/lib/python3.11/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/psf/lib/python3.11/site-packages/keras/engine/training.py:2349\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m   2341\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2342\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2343\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2346\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   2347\u001b[0m         )\n\u001b[0;32m-> 2349\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[1;32m   2350\u001b[0m     x\u001b[39m=\u001b[39mx,\n\u001b[1;32m   2351\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   2352\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39msteps,\n\u001b[1;32m   2353\u001b[0m     initial_epoch\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m   2354\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   2355\u001b[0m     max_queue_size\u001b[39m=\u001b[39mmax_queue_size,\n\u001b[1;32m   2356\u001b[0m     workers\u001b[39m=\u001b[39mworkers,\n\u001b[1;32m   2357\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39muse_multiprocessing,\n\u001b[1;32m   2358\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m   2359\u001b[0m     steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   2360\u001b[0m )\n\u001b[1;32m   2362\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2363\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/anaconda3/envs/psf/lib/python3.11/site-packages/keras/engine/data_adapter.py:1583\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1582\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1583\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/psf/lib/python3.11/site-packages/keras/engine/data_adapter.py:1260\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1257\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[1;32m   1259\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1260\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[1;32m   1261\u001b[0m     x,\n\u001b[1;32m   1262\u001b[0m     y,\n\u001b[1;32m   1263\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1264\u001b[0m     steps\u001b[39m=\u001b[39msteps_per_epoch,\n\u001b[1;32m   1265\u001b[0m     epochs\u001b[39m=\u001b[39mepochs \u001b[39m-\u001b[39m initial_epoch,\n\u001b[1;32m   1266\u001b[0m     sample_weights\u001b[39m=\u001b[39msample_weight,\n\u001b[1;32m   1267\u001b[0m     shuffle\u001b[39m=\u001b[39mshuffle,\n\u001b[1;32m   1268\u001b[0m     max_queue_size\u001b[39m=\u001b[39mmax_queue_size,\n\u001b[1;32m   1269\u001b[0m     workers\u001b[39m=\u001b[39mworkers,\n\u001b[1;32m   1270\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39muse_multiprocessing,\n\u001b[1;32m   1271\u001b[0m     distribution_strategy\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy(),\n\u001b[1;32m   1272\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m   1273\u001b[0m )\n\u001b[1;32m   1275\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   1277\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/psf/lib/python3.11/site-packages/keras/engine/data_adapter.py:348\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[39mreturn\u001b[39;00m flat_dataset\n\u001b[1;32m    346\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[0;32m--> 348\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice_inputs(indices_dataset, inputs)\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    352\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mshuffle_batch\u001b[39m(\u001b[39m*\u001b[39mbatch):\n",
      "File \u001b[0;32m~/anaconda3/envs/psf/lib/python3.11/site-packages/keras/engine/data_adapter.py:389\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrab_batch\u001b[39m(i, data):\n\u001b[1;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m    386\u001b[0m         \u001b[39mlambda\u001b[39;00m d: tf\u001b[39m.\u001b[39mgather(d, i, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), data\n\u001b[1;32m    387\u001b[0m     )\n\u001b[0;32m--> 389\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(grab_batch, num_parallel_calls\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE)\n\u001b[1;32m    391\u001b[0m \u001b[39m# Default optimizations are disabled to avoid the overhead of\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[39m# (unnecessary) input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[1;32m    393\u001b[0m options \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mOptions()\n",
      "File \u001b[0;32m~/anaconda3/envs/psf/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:2240\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2236\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2237\u001b[0m \u001b[39m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2238\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2239\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2240\u001b[0m \u001b[39mreturn\u001b[39;00m map_op\u001b[39m.\u001b[39m_map_v2(\n\u001b[1;32m   2241\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   2242\u001b[0m     map_func,\n\u001b[1;32m   2243\u001b[0m     num_parallel_calls\u001b[39m=\u001b[39mnum_parallel_calls,\n\u001b[1;32m   2244\u001b[0m     deterministic\u001b[39m=\u001b[39mdeterministic,\n\u001b[1;32m   2245\u001b[0m     name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/psf/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:40\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[39mreturn\u001b[39;00m _MapDataset(\n\u001b[1;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m   \u001b[39mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[1;32m     41\u001b[0m       input_dataset,\n\u001b[1;32m     42\u001b[0m       map_func,\n\u001b[1;32m     43\u001b[0m       num_parallel_calls\u001b[39m=\u001b[39mnum_parallel_calls,\n\u001b[1;32m     44\u001b[0m       deterministic\u001b[39m=\u001b[39mdeterministic,\n\u001b[1;32m     45\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m       name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/psf/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:163\u001b[0m, in \u001b[0;36m_ParallelMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_parallel_calls \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m    161\u001b[0m     num_parallel_calls, dtype\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mint64, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnum_parallel_calls\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n\u001b[0;32m--> 163\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mparallel_map_dataset_v2(\n\u001b[1;32m    164\u001b[0m     input_dataset\u001b[39m.\u001b[39m_variant_tensor,  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39mfunction\u001b[39m.\u001b[39mcaptured_inputs,\n\u001b[1;32m    166\u001b[0m     f\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39mfunction,\n\u001b[1;32m    167\u001b[0m     num_parallel_calls\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_parallel_calls,\n\u001b[1;32m    168\u001b[0m     deterministic\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deterministic,\n\u001b[1;32m    169\u001b[0m     use_inter_op_parallelism\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism,\n\u001b[1;32m    170\u001b[0m     preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preserve_cardinality,\n\u001b[1;32m    171\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_common_args)\n\u001b[1;32m    172\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[0;32m~/anaconda3/envs/psf/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:5784\u001b[0m, in \u001b[0;36mparallel_map_dataset_v2\u001b[0;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[1;32m   5782\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   5783\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 5784\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[1;32m   5785\u001b[0m       _ctx, \u001b[39m\"\u001b[39m\u001b[39mParallelMapDatasetV2\u001b[39m\u001b[39m\"\u001b[39m, name, input_dataset, other_arguments,\n\u001b[1;32m   5786\u001b[0m       num_parallel_calls, \u001b[39m\"\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m, f, \u001b[39m\"\u001b[39m\u001b[39moutput_types\u001b[39m\u001b[39m\"\u001b[39m, output_types,\n\u001b[1;32m   5787\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39moutput_shapes\u001b[39m\u001b[39m\"\u001b[39m, output_shapes, \u001b[39m\"\u001b[39m\u001b[39muse_inter_op_parallelism\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   5788\u001b[0m       use_inter_op_parallelism, \u001b[39m\"\u001b[39m\u001b[39mdeterministic\u001b[39m\u001b[39m\"\u001b[39m, deterministic,\n\u001b[1;32m   5789\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mpreserve_cardinality\u001b[39m\u001b[39m\"\u001b[39m, preserve_cardinality, \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m, metadata)\n\u001b[1;32m   5790\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   5791\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load model arrays\n",
    "\n",
    "N_components = [21, 24, 27, 30, 33]\n",
    "array_list= []\n",
    "\n",
    "for configuration in configurations[:4]:\n",
    "    model_array = load_model_array(N_components, 10, configuration)\n",
    "    predict_array(model_array)\n",
    "    array_list.append(model_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_metric(model_array, metric):\n",
    "    metric_array = np.zeros_like(model_array) \n",
    "\n",
    "    for i in range(model_array.shape[0]):\n",
    "        for j in range(model_array.shape[1]):\n",
    "            model = model_array[i][j]\n",
    "            metric_array[i][j] = getattr(model, metric)\n",
    "    return metric_array\n",
    "\n",
    "def obtain_graph_values(model_array, metric):\n",
    "    metric_array = obtain_metric(model_array, metric)\n",
    "\n",
    "    mean_values = np.mean(metric_array, axis=1).astype(float)\n",
    "    min_values = np.min(metric_array, axis=1).astype(float)\n",
    "    max_values = np.max(metric_array, axis=1).astype(float)\n",
    "    return mean_values, min_values, max_values\n",
    "\n",
    "def obtain_best_models(model_array):\n",
    "    best_models = np.empty((model_array.shape[0],1), dtype = object)\n",
    "\n",
    "    F1_array = obtain_metric(model_array, 'f1_mean_test')\n",
    "    indexes = np.argmax(F1_array, axis=1)\n",
    "    print(indexes)\n",
    "    for N in range(model_array.shape[0]):\n",
    "        model = model_array[N][indexes[N]]\n",
    "        best_models[N] = model\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the metrics of different configuration arrays with their error shape\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "np.random.seed(3)\n",
    "# Plotting the success rate\n",
    "for i, model_array in enumerate(array_list):\n",
    "    col = np.random.random(3,)\n",
    "    mean_valuesSt, minSt, maxSt = obtain_graph_values(model_array, 'success_rate_test')\n",
    "    mean_valuesF1t, minF1t, maxF1t = obtain_graph_values(model_array, 'f1_mean_test')\n",
    "\n",
    "    ax1.plot(N_components, mean_valuesSt, '-o', label='configuration '+str(i), color= col, linewidth=2)\n",
    "    ax1.fill_between(N_components, minSt, maxSt, alpha=0.3, color=col)\n",
    "\n",
    "    ax2.plot(N_components, mean_valuesF1t, '-o', label='configuration '+str(i), color=col, linewidth=2)\n",
    "    ax2.fill_between(N_components, minF1t, maxF1t, alpha=0.3, color=col)\n",
    "\n",
    "\n",
    "ax1.set_xlabel(\"PCA components\", fontsize=18)\n",
    "ax1.set_ylabel(\"Success Rate\", fontsize=18)\n",
    "ax1.set_xticks(N_components)\n",
    "ax1.set_yticks([0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "ax1.tick_params(axis='x', labelsize=15)\n",
    "ax1.tick_params(axis='y', labelsize=15)\n",
    "ax1.grid(True)\n",
    "ax1.legend(fontsize=12)\n",
    "\n",
    "# Plotting the F1 score\n",
    "\n",
    "ax2.set_xlabel(\"PCA components\", fontsize=18)\n",
    "ax2.set_ylabel(\"F1 Score\", fontsize=18)\n",
    "ax2.set_xticks(N_components)\n",
    "ax2.set_yticks([0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "ax2.tick_params(axis='x', labelsize=15)\n",
    "ax2.tick_params(axis='y', labelsize=15)\n",
    "ax2.grid(True)\n",
    "ax2.legend(fontsize=12)\n",
    "\n",
    "# Set the title for the entire figure\n",
    "fig.suptitle('Performance of the classifier in function of the number of PCA components', fontsize=20)\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "#plt.subplots_adjust(hspace=1.3)\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
